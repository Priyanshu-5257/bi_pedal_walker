{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d84e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.losses import MSE\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3403ba61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b039078d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env=gym.make(\"BipedalWalker-v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a84d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = env.observation_space.shape\n",
    "num_action = 4\n",
    "print(state_size , num_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e2ce9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_network = Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=state_size),\n",
    "    Dense(256,activation='relu'),\n",
    "    Dense(128,activation='relu'),\n",
    "    Dense(64,activation='relu'),\n",
    "    Dense(num_action)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96afd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_q_net = Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=state_size),\n",
    "    Dense(256,activation='relu'),\n",
    "    Dense(128,activation='relu'),\n",
    "    Dense(64,activation='relu'),\n",
    "    Dense(num_action)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4bb976",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d215cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import deque, namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b21686",
   "metadata": {},
   "outputs": [],
   "source": [
    "experience = namedtuple(\"Experience\",field_names=['state','action','reward','next_state','done'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fde265",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(experiences,gamma,q_network,target_q_net):\n",
    "    \n",
    "    \n",
    "    states,actions,rewards,next_states,done=experiences\n",
    "    max_qsa=tf.reduce_max(target_q_net(next_states),axis=-1)\n",
    "    y_target=rewards + (max_qsa*gamma)*(1-done)\n",
    "    q_values=q_network(states)\n",
    "    q_values = tf.gather_nd(q_values, tf.stack([tf.range(q_values.shape[0]),\n",
    "                                                tf.cast(tf.argmax(actions,axis=-1), tf.int32)], axis=1))\n",
    "    loss=MSE(y_target,q_values)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e64c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target_net(q_network,target_q_net):\n",
    "    for target_weights,q_net_weights in zip(target_q_net.weights,q_network.weights):        \n",
    "        target_weights.assign((1-up_lr)*target_weights + (up_lr)*q_net_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8808df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_learn(experiences,gamma):\n",
    "    with tf.GradientTape() as tape :\n",
    "        loss = compute_loss(experiences,gamma,q_network,target_q_net)\n",
    "    gradients = tape.gradient(loss,q_network.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients,q_network.trainable_variables))\n",
    "    update_target_net(q_network,target_q_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88d345f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def get_experiences(memory_buffer):\n",
    "    experiences = random.sample(memory_buffer, k=64)\n",
    "    states = tf.convert_to_tensor(\n",
    "        np.array([e.state for e in experiences if e is not None]), dtype=tf.float32\n",
    "    )\n",
    "    actions = tf.convert_to_tensor(\n",
    "        np.array([e.action for e in experiences if e is not None]), dtype=tf.float32\n",
    "    )\n",
    "    rewards = tf.convert_to_tensor(\n",
    "        np.array([e.reward for e in experiences if e is not None]), dtype=tf.float32\n",
    "    )\n",
    "    next_states = tf.convert_to_tensor(\n",
    "        np.array([e.next_state for e in experiences if e is not None]), dtype=tf.float32\n",
    "    )\n",
    "    done_vals = tf.convert_to_tensor(\n",
    "        np.array([e.done for e in experiences if e is not None]).astype(np.uint8),\n",
    "        dtype=tf.float32,\n",
    "    )\n",
    "    return(states,actions,rewards,next_states,done_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd00a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_upd(t,num_steps_upd,memory_buffer):\n",
    "    if (t+1)%num_steps_upd ==0 and len(memory_buffer) > 64:\n",
    "        return True\n",
    "    else :\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de59650e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_eps(eps):\n",
    "    return np.max([0.05,0.95*eps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f23c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(action,eps):\n",
    "    if random.random() > eps :\n",
    "        act=np.zeros(4)\n",
    "        i=np.argmax(action)\n",
    "        act[i]=np.tanh(action[0][i])\n",
    "        \n",
    "        \n",
    "        return act\n",
    "    else :\n",
    "        action=np.random.default_rng().uniform(-1,1,size=(4,))\n",
    "        act=np.zeros(4)\n",
    "        i=np.argmax(action)\n",
    "        act[i]=np.tanh(action[i])\n",
    "        return act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a080efca",
   "metadata": {},
   "outputs": [],
   "source": [
    "MEMORY_SIZE = 100_000     \n",
    "gamma = 0.995             \n",
    "ALPHA = 1e-3              \n",
    "NUM_STEPS_FOR_UPDATE = 4\n",
    "up_lr=0.01\n",
    "threshold=0\n",
    "lr=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16a92ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "num_eps = 200000\n",
    "max_num_timesteps = 100\n",
    "total_point_history = []\n",
    "\n",
    "num_p_av = 100\n",
    "eps=0.10\n",
    "memory_buffer = deque(maxlen=MEMORY_SIZE)\n",
    "\n",
    "target_q_net.set_weights(q_network.get_weights())\n",
    "\n",
    "for i in range(num_eps):\n",
    "    env=gym.make(\"BipedalWalker-v3\")\n",
    "    state=env.reset()\n",
    "    total_points=0\n",
    "\n",
    "    for t in range(max_num_timesteps):\n",
    "        #env.render()\n",
    "        state_qn = np.expand_dims(state,axis=0)\n",
    "        q_values = q_network(state_qn)\n",
    "        action = get_action(q_values,eps)\n",
    "\n",
    "        next_state,reward,done,_ = env.step(action)\n",
    "        memory_buffer.append(experience(state,action,reward,next_state,done))\n",
    "\n",
    "        update = check_upd(t,NUM_STEPS_FOR_UPDATE,memory_buffer)\n",
    "\n",
    "        \n",
    "        if update :\n",
    "            experiences = get_experiences(memory_buffer)\n",
    "            agent_learn(experiences,gamma)\n",
    "        state= next_state.copy()\n",
    "        total_points += reward\n",
    "\n",
    "        if done :\n",
    "            break\n",
    "        total_point_history.append(total_points)\n",
    "        av_latest_points = np.mean(total_point_history[-num_p_av:])\n",
    "        eps = get_new_eps(eps)\n",
    "    env.close()\n",
    "\n",
    "    print(f\"\\rEpisode {i+1} | Total point average of the last {num_p_av} episodes: {av_latest_points:.2f}\", end=\"\")\n",
    "    if (i+1) % num_p_av == 0:\n",
    "        print(f\"\\rEpisode {i+1} | Total point average of the last {num_p_av} episodes: {av_latest_points:.2f}\")\n",
    "    if av_latest_points >= threshold:\n",
    "        max_num_timesteps = min(50+max_num_timesteps,1600)\n",
    "        threshold=av_latest_points\n",
    "        if av_latest_points >300 :\n",
    "            q_network.save('bipedalwalkwer_20/02.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be90074f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
